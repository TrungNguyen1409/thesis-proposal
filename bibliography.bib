@misc{han2025llmmultiagentsystemschallenges,
      title={LLM Multi-Agent Systems: Challenges and Open Problems}, 
      author={Shanshan Han and Qifan Zhang and Yuhang Yao and Weizhao Jin and Zhaozhuo Xu},
      year={2025},
      eprint={2402.03578},
      archivePrefix={arXiv},
      primaryClass={cs.MA},
      url={https://arxiv.org/abs/2402.03578}, 
}

@misc{shu2024effectivegenaimultiagentcollaboration,
      title={Towards Effective GenAI Multi-Agent Collaboration: Design and Evaluation for Enterprise Applications}, 
      author={Raphael Shu and Nilaksh Das and Michelle Yuan and Monica Sunkara and Yi Zhang},
      year={2024},
      eprint={2412.05449},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.05449}, 
}


@misc{belcak2025smalllanguagemodelsfuture,
      title={Small Language Models are the Future of Agentic AI}, 
      author={Peter Belcak and Greg Heinrich and Shizhe Diao and Yonggan Fu and Xin Dong and Saurav Muralidharan and Yingyan Celine Lin and Pavlo Molchanov},
      year={2025},
      eprint={2506.02153},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2506.02153}, 
}

@misc{wang2024mixtureofagentsenhanceslargelanguage,
      title={Mixture-of-Agents Enhances Large Language Model Capabilities}, 
      author={Junlin Wang and Jue Wang and Ben Athiwaratkun and Ce Zhang and James Zou},
      year={2024},
      eprint={2406.04692},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.04692}, 
}

@misc{wu2023autogenenablingnextgenllm,
      title={AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation}, 
      author={Qingyun Wu and Gagan Bansal and Jieyu Zhang and Yiran Wu and Beibin Li and Erkang Zhu and Li Jiang and Xiaoyun Zhang and Shaokun Zhang and Jiale Liu and Ahmed Hassan Awadallah and Ryen W White and Doug Burger and Chi Wang},
      year={2023},
      eprint={2308.08155},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2308.08155}, 
}

@misc{wikipedia2025contractlifecyclemanagement,
      title={Contract lifecycle management}, 
      author={{Wikipedia contributors}},
      year={2025},
      url={https://en.wikipedia.org/wiki/Contract_lifecycle_management},
      note={Last edited on 5 September 2025, at 06:44 (UTC)},
}

@inproceedings{Mohammadi_2025, series={KDD '25},
   title={Evaluation and Benchmarking of LLM Agents: A Survey},
   url={http://dx.doi.org/10.1145/3711896.3736570},
   DOI={10.1145/3711896.3736570},
   booktitle={Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2},
   publisher={ACM},
   author={Mohammadi, Mahmoud and Li, Yipeng and Lo, Jane and Yip, Wendy},
   year={2025},
   month=aug, pages={6129–6139},
   collection={KDD '25} }

@misc{yehudai2025llmagentevaluationregulated,
      title={LLM Agent Evaluation in Regulated Enterprise Domains: A Comprehensive Survey},
      author={Yehudai, A. and Chen, L. and Rodriguez, M. and Kim, S.},
      year={2025},
      eprint={2503.16416},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2503.16416},
}


@misc{billi2023largelanguagemodelsexplainable,
      title={Large Language Models and Explainable Law: a Hybrid Methodology}, 
      author={Marco Billi and Alessandro Parenti and Giuseppe Pisano and Marco Sanchi},
      year={2023},
      eprint={2311.11811},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2311.11811}, 
}

@misc{xia2025parallelismmeetsadaptivenessscalable,
      title={Parallelism Meets Adaptiveness: Scalable Documents Understanding in Multi-Agent LLM Systems}, 
      author={Chengxuan Xia and Qianye Wu and Sixuan Tian and Yilun Hao},
      year={2025},
      eprint={2507.17061},
      archivePrefix={arXiv},
      primaryClass={cs.MA},
      url={https://arxiv.org/abs/2507.17061}, 
}

@article{blackboxalgorithms,
author = {Gryz, Jarek and Rojszczak, Marcin},
year = {2021},
month = {06},
pages = {},
title = {Black box algorithms and the rights of individuals: No easy solution to the “explainability” problem},
volume = {10},
journal = {Internet Policy Review},
doi = {10.14763/2021.2.1564}
}

@misc{wang2024legalevalutionschallengeslarge,
      title={Legal Evalutions and Challenges of Large Language Models}, 
      author={Jiaqi Wang and Huan Zhao and Zhenyuan Yang and Peng Shu and Junhao Chen and Haobo Sun and Ruixi Liang and Shixin Li and Pengcheng Shi and Longjun Ma and Zongjia Liu and Zhengliang Liu and Tianyang Zhong and Yutong Zhang and Chong Ma and Xin Zhang and Tuo Zhang and Tianli Ding and Yudan Ren and Tianming Liu and Xi Jiang and Shu Zhang},
      year={2024},
      eprint={2411.10137},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.10137}, 
}

@inproceedings{hallucinationlegal,
author = {Bal\i{}, Yavuz and \c{C}ilo\u{g}lugil, Birol},
title = {Leveraging Large Language Models for Natural Language Processing Based Tasks in the Legal Domain: A Short Survey},
year = {2025},
isbn = {978-3-031-96996-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-96997-3_11},
doi = {10.1007/978-3-031-96997-3_11},
abstract = {The integration of natural language processing techniques (NLP) into the legal domain has opened up new possibilities for automating, analyzing, and improving access to legal information. However, the complex and specialized terminology of legal texts has always been a significant challenge for the NLP area. To overcome this challenge, research studies in the law domain conducted with NLP techniques can be categorized into the following tasks: classification, information extraction, summarization, information retrieval, legal judgment prediction. Recent advancements in large language models (LLMs) have significantly contributed to NLP tasks across various domains, including the aforementioned tasks in the legal domain. The ability of large language models to reason and understand the deep context of texts has shown promising capabilities for understanding legal language and overcoming legal challenges. Therefore, this paper explores the existing tasks addressed by NLP in the legal domain and delves into LLM-based approaches which address these NLP tasks. Moreover, the datasets and evaluation criteria for LLMs have been examined, and the challenges and key limitations of LLMs in the legal domain have been identified. Hallucination, lack of domain-specific knowledge, and lack of interpretability have been determined as the challenges for the reliable application of large language models in the legal field. Finally, opportunities for future research have been discussed.},
booktitle = {Computational Science and Its Applications – ICCSA 2025: 25th International Conference, Istanbul, Turkey, June 30–July 3, 2025, Proceedings, Part II},
pages = {166–178},
numpages = {13},
keywords = {Law, Legal Domain, Large Language Models, Natural Language Processing},
location = {Istanbul, T\"{u}rkiye}
}

@misc{wang2025lmarslegalmultiagentworkflow,
      title={L-MARS: Legal Multi-Agent Workflow with Orchestrated Reasoning and Agentic Search}, 
      author={Ziqi Wang and Boqin Yuan},
      year={2025},
      eprint={2509.00761},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2509.00761}, 
}

@misc{kargwal2025masteringmultiagenteval,
      title={Mastering Multi-Agent Eval Systems in 2025}, 
      author={Aryan Kargwal},
      year={2025},
      month={January},
      url={https://botpress.com/blog/multi-agent-evaluation-systems},
      note={Botpress Blog},
      publisher={Botpress},
      howpublished={Blog post}
}